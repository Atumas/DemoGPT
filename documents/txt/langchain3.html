<div class="inner"><div class="hero2__content2"></div></div></div><div class="post"><div class="post__inner nlp inner"><div class="post__content content__inner"><h1>Conversational Memory for LLMs with Langchain</h1><p>Conversational memory is how a chatbot can respond to multiple queries in a chat-like manner. It enables a coherent conversation, and without it, every query would be treated as an entirely independent input without considering past interactions.</p><p><img loading="lazy" src="./langchain3_files/langchain-conversational-memory-1.png" alt="With and without conversational memory" width="100%">
<small>The LLM with and without conversational memory. The blue boxes are user prompts and in grey are the LLMs responses. Without conversational memory (right), the LLM cannot respond using knowledge of previous interactions.</small></p><p>The memory allows a <strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odel (LLM) to remember previous interactions with the user. By default, LLMs are <em>stateless</em> — meaning each incoming query is processed independently of other interactions. The only thing that exists for a stateless agent is the current input, nothing else.</p><p>There are many applications where remembering previous interactions is very important, such as chatbots. Conversational memory allows us to do that.</p><p>There are several ways that we can implement conversational memory. In the context of [LangChain](/learn/langchain-intro/, they are all built on top of the <code>ConversationChain</code>.</p><div style="left:0;width:100%;height:0;position:relative;padding-bottom:56.25%"><iframe style="border:1;top:0;left:0;width:100%;height:100%;position:absolute" src="./langchain3_files/X05uK0TZozM.html" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><hr><h2 id="conversationchain">ConversationChain</h2><p>We can start by initializing the <code>ConversationChain</code>. We will use OpenAI’s <code>text-davinci-003</code> as the LLM, but other models like <code>gpt-3.5-turbo</code> can be used.</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> OpenAI
<span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains <span class="token keyword">import</span> ConversationChain

<span class="token comment"># first initialize the large language model</span>
llm <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>
	temperature<span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">,</span>
	openai_api_key<span class="token operator">=</span><span class="token string">"OPENAI_API_KEY"</span><span class="token punctuation">,</span>
	model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span>
<span class="token punctuation">)</span>

<span class="token comment"># now initialize the conversation chain</span>
conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>We can see the prompt template used by the <code>ConversationChain</code> like so:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[8]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>conversation<span class="token punctuation">.</span>prompt<span class="token punctuation">.</span>template<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[8]:</div><div class="output"><pre><code>The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.
</code></pre><pre><code>
</code></pre><pre><code>Current conversation:
</code></pre><pre><code>{history}
</code></pre><pre><code>Human: {input}
</code></pre><pre><code>AI:
</code></pre></div></div></div><p>Here, the prompt primes the model by telling it that the following is a conversation between a human (us) and an AI (<code>text-davinci-003</code>). The prompt attempts to reduce <em>hallucinations</em> (where a model makes things up) by stating:</p><pre><code>"If the AI does not know the answer to a question, it truthfully says it does not know."
</code></pre><p>This can help but does not solve the problem of hallucinations — but we will save this for the topic of a future chapter.</p><div class="shortcode-form"><form class="form js-form" data-ga="Newsletter" data-id="74adb696-e56a-47b3-9911-f249db88a172" data-portal="8231564" method="post" name="newsletter"><input type="hidden" name="form-name" value="newsletter"><fieldset><div hidden=""><label>Don't fill this out if you're human: <input name="bot-field"></label></div><label class="shortcode-form-label" for="newsletter-email">Get an update when we release future chatpers!</label>
<span class="form-group"><input type="email" name="email" id="newsletter-email" placeholder="Email address..." required="">
<button class="btn" type="submit">Submit</button></span>
<input type="hidden" name="lifecyclestage" id="lifecyclestage" value="subscriber"><div class="form__info"><div class="js-form-ok" role="alert" hidden="">Subscribed successfully.</div><div class="js-form-not-ok" role="alert" hidden="">Failed to submit.</div></div></fieldset></form></div><p>Following the initial prompt, we see two parameters; <code>{history}</code> and <code>{input}</code>. The <code>{input}</code> is where we’d place the latest human query; it is the input entered into a chatbot text box:</p><p><img loading="lazy" src="./langchain3_files/langchain-conversational-memory-2.png" alt="Screenshot from ChatGPT conversation showing chat history and input" width="100%"></p><p>The <code>{history}</code> is where conversational memory is used. Here, we feed in information about the conversation history between the human and AI.</p><p>These two parameters — <code>{history}</code> and <code>{input}</code> — are passed to the LLM within the prompt template we just saw, and the output that we (hopefully) return is simply the predicted continuation of the conversation.</p><h2 id="forms-of-conversational-memory">Forms of Conversational Memory</h2><p>We can use several types of conversational memory with the <code>ConversationChain</code>. They modify the text passed to the <code>{history}</code> parameter.</p><h3 id="conversationbuffermemory">ConversationBufferMemory</h3><p><em>(Follow along with our <a href="https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/03-langchain-conversational-memory.ipynb">Jupyter notebooks</a>)</em></p><p>The <code>ConversationBufferMemory</code> is the most straightforward conversational memory in LangChain. As we described above, the raw input of the past conversation between the human and AI is passed — in its raw form — to the <code>{history}</code> parameter.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[11]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains<span class="token punctuation">.</span>conversation<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferMemory

conversation_buf <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>
    llm<span class="token operator">=</span>llm<span class="token punctuation">,</span>
    memory<span class="token operator">=</span>ConversationBufferMemory<span class="token punctuation">(</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[32]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">conversation_buf<span class="token punctuation">(</span><span class="token string">"Good morning AI!"</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[32]:</div><div class="output"><pre><code>{'input': 'Good morning AI!',
</code></pre><pre><code> 'history': '',
</code></pre><pre><code> 'response': " Good morning! It's a beautiful day today, isn't it? How can I help you?"}</code></pre></div></div></div><p>We return the first response from the conversational agent. Let’s continue the conversation, writing prompts that the LLM can only answer <em>if</em> it considers the conversation history. We also add a <code>count_tokens</code> function so we can see how many tokens are being used by each interaction.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[6]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>callbacks <span class="token keyword">import</span> get_openai_callback

<span class="token keyword">def</span> <span class="token function">count_tokens</span><span class="token punctuation">(</span>chain<span class="token punctuation">,</span> query<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">with</span> get_openai_callback<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> cb<span class="token punctuation">:</span>
        result <span class="token operator">=</span> chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>query<span class="token punctuation">)</span>
        <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string-interpolation"><span class="token string">f'Spent a total of </span><span class="token interpolation"><span class="token punctuation">{</span>cb<span class="token punctuation">.</span>total_tokens<span class="token punctuation">}</span></span><span class="token string"> tokens'</span></span><span class="token punctuation">)</span>

    <span class="token keyword">return</span> result<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[33]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_buf<span class="token punctuation">,</span> 
    <span class="token string">"My interest here is to explore the potential of integrating Large Language Models with external knowledge"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[33]:</div><div class="output"><pre><code>Spent a total of 179 tokens
</code></pre><pre><code>' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Is there anything else I can help you with?'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[34]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_buf<span class="token punctuation">,</span>
    <span class="token string">"I just want to analyze the different possibilities. What can you think of?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[34]:</div><div class="output"><pre><code>Spent a total of 268 tokens
</code></pre><pre><code>' Well, integrating Large Language Models with external knowledge can open up a lot of possibilities. For example, you could use them to generate more accurate and detailed summaries of text, or to answer questions about a given context more accurately. You could also use them to generate more accurate translations, or to generate more accurate predictions about future events.'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[35]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_buf<span class="token punctuation">,</span> 
    <span class="token string">"Which data source types could be used to give context to the model?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[35]:</div><div class="output"><pre><code>Spent a total of 360 tokens
</code></pre><pre><code>'  There are a variety of data sources that could be used to give context to a Large Language Model. These include structured data sources such as databases, unstructured data sources such as text documents, and even audio and video data sources. Additionally, you could use external knowledge sources such as Wikipedia or other online encyclopedias to provide additional context.'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[36]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_buf<span class="token punctuation">,</span> 
    <span class="token string">"What is my aim again?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[36]:</div><div class="output"><pre><code>Spent a total of 388 tokens
</code></pre><pre><code>' Your aim is to explore the potential of integrating Large Language Models with external knowledge.'</code></pre></div></div></div><p>The LLM can clearly remember the history of the conversation. Let’s take a look at <em>how</em> this conversation history is stored by the <code>ConversationBufferMemory</code>:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[37]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>conversation_buf<span class="token punctuation">.</span>memory<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[37]:</div><div class="output"><pre><code>
</code></pre><pre><code>Human: Good morning AI!
</code></pre><pre><code>AI:  Good morning! It's a beautiful day today, isn't it? How can I help you?
</code></pre><pre><code>Human: My interest here is to explore the potential of integrating Large Language Models with external knowledge
</code></pre><pre><code>AI:  Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Is there anything else I can help you with?
</code></pre><pre><code>Human: I just want to analyze the different possibilities. What can you think of?
</code></pre><pre><code>AI:  Well, integrating Large Language Models with external knowledge can open up a lot of possibilities. For example, you could use them to generate more accurate and detailed summaries of text, or to answer questions about a given context more accurately. You could also use them to generate more accurate translations, or to generate more accurate predictions about future events.
</code></pre><pre><code>Human: Which data source types could be used to give context to the model?
</code></pre><pre><code>AI:   There are a variety of data sources that could be used to give context to a Large Language Model. These include structured data sources such as databases, unstructured data sources such as text documents, and even audio and video data sources. Additionally, you could use external knowledge sources such as Wikipedia or other online encyclopedias to provide additional context.
</code></pre><pre><code>Human: What is my aim again?
</code></pre><pre><code>AI:  Your aim is to explore the potential of integrating Large Language Models with external knowledge.
</code></pre></div></div></div><p>We can see that the buffer saves every interaction in the chat history directly. There are a few pros and cons to this approach. In short, they are:</p><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Storing everything gives the LLM the maximum amount of information</td><td>More tokens mean slowing response times and higher costs</td></tr><tr><td>Storing everything is simple and intuitive</td><td>Long conversations cannot be remembered as we hit the LLM token limit (<code>4096</code> tokens for <code>text-davinci-003</code> and <code>gpt-3.5-turbo</code>)</td></tr></tbody></table><p>The <code>ConversationBufferMemory</code> is an excellent option to get started with but is limited by the storage of every interaction. Let’s take a look at other options that help remedy this.</p><h3 id="conversationsummarymemory">ConversationSummaryMemory</h3><p>Using <code>ConversationBufferMemory</code>, we very quickly use <em>a lot</em> of tokens and even exceed the context window limit of even the most advanced LLMs available today.</p><p>To avoid excessive token usage, we can use <code>ConversationSummaryMemory</code>. As the name would suggest, this form of memory <em>summarizes</em> the conversation history before it is passed to the <code>{history}</code> parameter.</p><p>We initialize the <code>ConversationChain</code> with the summary memory like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains<span class="token punctuation">.</span>conversation<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationSummaryMemory

conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>
	llm<span class="token operator">=</span>llm<span class="token punctuation">,</span>
	memory<span class="token operator">=</span>ConversationSummaryMemory<span class="token punctuation">(</span>llm<span class="token operator">=</span>llm<span class="token punctuation">)</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>When using <code>ConversationSummaryMemory</code>, we need to pass an LLM to the object because the summarization is powered by an LLM. We can see the prompt used to do this here:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[19]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>conversation_sum<span class="token punctuation">.</span>memory<span class="token punctuation">.</span>prompt<span class="token punctuation">.</span>template<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[19]:</div><div class="output"><pre><code>Progressively summarize the lines of conversation provided, adding onto the previous summary returning a new summary.
</code></pre><pre><code>
</code></pre><pre><code>EXAMPLE
</code></pre><pre><code>Current summary:
</code></pre><pre><code>The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good.
</code></pre><pre><code>
</code></pre><pre><code>New lines of conversation:
</code></pre><pre><code>Human: Why do you think artificial intelligence is a force for good?
</code></pre><pre><code>AI: Because artificial intelligence will help humans reach their full potential.
</code></pre><pre><code>
</code></pre><pre><code>New summary:
</code></pre><pre><code>The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.
</code></pre><pre><code>END OF EXAMPLE
</code></pre><pre><code>
</code></pre><pre><code>Current summary:
</code></pre><pre><code>{summary}
</code></pre><pre><code>
</code></pre><pre><code>New lines of conversation:
</code></pre><pre><code>{new_lines}
</code></pre><pre><code>
</code></pre><pre><code>New summary:
</code></pre></div></div></div><p>Using this, we can summarize every new interaction and append it to a “running summary” of all past interactions. Let’s have another conversation utilizing this approach.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[40]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token comment"># without count_tokens we'd call `conversation_sum("Good morning AI!")`</span>
<span class="token comment"># but let's keep track of our tokens:</span>
count_tokens<span class="token punctuation">(</span>
    conversation_sum<span class="token punctuation">,</span> 
    <span class="token string">"Good morning AI!"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[40]:</div><div class="output"><pre><code>Spent a total of 290 tokens
</code></pre><pre><code>" Good morning! It's a beautiful day today, isn't it? How can I help you?"</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[41]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_sum<span class="token punctuation">,</span> 
    <span class="token string">"My interest here is to explore the potential of integrating Large Language Models with external knowledge"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[41]:</div><div class="output"><pre><code>Spent a total of 440 tokens
</code></pre><pre><code>" That sounds like an interesting project! I'm familiar with Large Language Models, but I'm not sure how they could be integrated with external knowledge. Could you tell me more about what you have in mind?"</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[42]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_sum<span class="token punctuation">,</span> 
    <span class="token string">"I just want to analyze the different possibilities. What can you think of?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[42]:</div><div class="output"><pre><code>Spent a total of 664 tokens
</code></pre><pre><code>' I can think of a few possibilities. One option is to use a large language model to generate a set of candidate answers to a given query, and then use external knowledge to filter out the most relevant answers. Another option is to use the large language model to generate a set of candidate answers, and then use external knowledge to score and rank the answers. Finally, you could use the large language model to generate a set of candidate answers, and then use external knowledge to refine the answers.'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[43]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_sum<span class="token punctuation">,</span> 
    <span class="token string">"Which data source types could be used to give context to the model?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[43]:</div><div class="output"><pre><code>Spent a total of 799 tokens
</code></pre><pre><code>' There are many different types of data sources that could be used to give context to the model. These could include structured data sources such as databases, unstructured data sources such as text documents, or even external APIs that provide access to external knowledge. Additionally, the model could be trained on a combination of these data sources to provide a more comprehensive understanding of the context.'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[44]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_sum<span class="token punctuation">,</span> 
    <span class="token string">"What is my aim again?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[44]:</div><div class="output"><pre><code>Spent a total of 853 tokens
</code></pre><pre><code>' Your aim is to explore the potential of integrating Large Language Models with external knowledge.'</code></pre></div></div></div><p>In this case the summary contains enough information for the LLM to “remember” our original aim. We can see this summary in it’s raw form like so:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[45]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>conversation_sum<span class="token punctuation">.</span>memory<span class="token punctuation">.</span><span class="token builtin">buffer</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[45]:</div><div class="output"><pre><code>
</code></pre><pre><code>The human greeted the AI with a good morning, to which the AI responded with a good morning and asked how it could help. The human expressed interest in exploring the potential of integrating Large Language Models with external knowledge, to which the AI responded positively and asked for more information. The human asked the AI to think of different possibilities, and the AI suggested three options: using the large language model to generate a set of candidate answers and then using external knowledge to filter out the most relevant answers, score and rank the answers, or refine the answers. The human then asked which data source types could be used to give context to the model, to which the AI responded that there are many different types of data sources that could be used, such as structured data sources, unstructured data sources, or external APIs. Additionally, the model could be trained on a combination of these data sources to provide a more comprehensive understanding of the context. The human then asked what their aim was again, to which the AI responded that their aim was to explore the potential of integrating Large Language Models with external knowledge.
</code></pre></div></div></div><p>The number of tokens being used for this conversation is greater than when using the <code>ConversationBufferMemory</code>, so is there any advantage to using <code>ConversationSummmaryMemory</code> over the buffer memory?</p><p><img loading="lazy" src="./langchain3_files/langchain-conversational-memory-3.png" alt="Token count as number of interactions increase" width="100%">
<small>Token count (y-axis) for the buffer memory vs. summary memory as the number of interactions (x-axis) increases.</small></p><p>For longer conversations, yes. <a href="https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/03a-token-counter.ipynb">Here</a>, we have a longer conversation. As shown above, the summary memory initially uses far more tokens. However, as the conversation progresses, the summarization approach grows more slowly. In contrast, the buffer memory continues to grow linearly with the number of tokens in the chat.</p><p>We can summarize the pros and cons of <code>ConversationSummaryMemory</code> as follows:</p><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Shortens the number of tokens for <em>long</em> conversations.</td><td>Can result in higher token usage for smaller conversations</td></tr><tr><td>Enables much longer conversations</td><td>Memorization of the conversation history is wholly reliant on the summarization ability of the intermediate summarization LLM</td></tr><tr><td>Relatively straightforward implementation, intuitively simple to understand</td><td>Also requires token usage for the summarization LLM; this increases costs (but does not limit conversation length)</td></tr></tbody></table><p>Conversation summarization is a good approach for cases where long conversations are expected. Yet, it is still fundamentally limited by token limits. After a certain amount of time, we still exceed context window limits.</p><h3 id="conversationbufferwindowmemory">ConversationBufferWindowMemory</h3><p>The <code>ConversationBufferWindowMemory</code> acts in the same way as our earlier <em>“buffer memory”</em> but adds a <em>window</em> to the memory. Meaning that we only keep a given number of past interactions before <em>“forgetting”</em> them. We use it like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>chains<span class="token punctuation">.</span>conversation<span class="token punctuation">.</span>memory <span class="token keyword">import</span> ConversationBufferWindowMemory

conversation <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>
	llm<span class="token operator">=</span>llm<span class="token punctuation">,</span>
	memory<span class="token operator">=</span>ConversationBufferWindowMemory<span class="token punctuation">(</span>k<span class="token operator">=</span><span class="token number">1</span><span class="token punctuation">)</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>In this instance, we set <code>k=1</code> — this means the window will remember the single latest interaction between the human and AI. That is the latest human response and the latest AI response. We can see the effect of this below:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[61]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_bufw<span class="token punctuation">,</span> 
    <span class="token string">"Good morning AI!"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[61]:</div><div class="output"><pre><code>Spent a total of 85 tokens
</code></pre><pre><code>" Good morning! It's a beautiful day today, isn't it? How can I help you?"</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[62]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_bufw<span class="token punctuation">,</span> 
    <span class="token string">"My interest here is to explore the potential of integrating Large Language Models with external knowledge"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[62]:</div><div class="output"><pre><code>Spent a total of 178 tokens
</code></pre><pre><code>' Interesting! Large Language Models are a type of artificial intelligence that can process natural language and generate text. They can be used to generate text from a given context, or to answer questions about a given context. Integrating them with external knowledge can help them to better understand the context and generate more accurate results. Do you have any specific questions about this integration?'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[63]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_bufw<span class="token punctuation">,</span> 
    <span class="token string">"I just want to analyze the different possibilities. What can you think of?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[63]:</div><div class="output"><pre><code>Spent a total of 233 tokens
</code></pre><pre><code>' There are many possibilities for integrating Large Language Models with external knowledge. For example, you could use external knowledge to provide additional context to the model, or to provide additional training data. You could also use external knowledge to help the model better understand the context of a given text, or to help it generate more accurate results.'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[64]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_bufw<span class="token punctuation">,</span> 
    <span class="token string">"Which data source types could be used to give context to the model?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[64]:</div><div class="output"><pre><code>Spent a total of 245 tokens
</code></pre><pre><code>' Data sources that could be used to give context to the model include text corpora, structured databases, and ontologies. Text corpora provide a large amount of text data that can be used to train the model and provide additional context. Structured databases provide structured data that can be used to provide additional context to the model. Ontologies provide a structured representation of knowledge that can be used to provide additional context to the model.'</code></pre></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[65]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">count_tokens<span class="token punctuation">(</span>
    conversation_bufw<span class="token punctuation">,</span> 
    <span class="token string">"What is my aim again?"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[65]:</div><div class="output"><pre><code>Spent a total of 186 tokens
</code></pre><pre><code>' Your aim is to use data sources to give context to the model.'</code></pre></div></div></div><p>By the end of the conversation, when we ask <code>"What is my aim again?"</code>, the answer to this was contained in the human response <em>three</em> interactions ago. As we only kept the most recent interaction (<code>k=1</code>), the model had forgotten and could not give the correct answer.</p><p>We can see the effective “memory” of the model like so:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[66]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">bufw_history <span class="token operator">=</span> conversation_bufw<span class="token punctuation">.</span>memory<span class="token punctuation">.</span>load_memory_variables<span class="token punctuation">(</span>
    inputs<span class="token operator">=</span><span class="token punctuation">[</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token string">'history'</span><span class="token punctuation">]</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[67]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>bufw_history<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[67]:</div><div class="output"><pre><code>Human: What is my aim again?
</code></pre><pre><code>AI:  Your aim is to use data sources to give context to the model.
</code></pre></div></div></div><p>Although this method isn’t suitable for remembering distant interactions, it is good at limiting the number of tokens being used — a number that we can increase/decrease depending on our needs. For the <a href="https://github.com/pinecone-io/examples/blob/master/generation/langchain/handbook/03a-token-counter.ipynb">longer conversation</a> used in our earlier comparison, we can set <code>k=6</code> and reach ~1.5K tokens per interaction after 27 total interactions:</p><p><img loading="lazy" src="./langchain3_files/langchain-conversational-memory-4.png" alt="Token count comparison including buffer window memories" width="100%">
<small>Token count including the <code>ConversationBufferWindowMemory</code> at <code>k=6</code> and <code>k=12</code>.</small></p><p>If we only need memory of recent interactions, this is a great option. However, for a mix of both distant and recent interactions, there are other options.</p><h3 id="conversationsummarybuffermemory">ConversationSummaryBufferMemory</h3><p>The <code>ConversationSummaryBufferMemory</code> is a mix of the <code>ConversationSummaryMemory</code> and the <code>ConversationBufferWindowMemory</code>. It summarizes the earliest interactions in a conversation while maintaining the <code>max_token_limit</code> most recent tokens in their conversation. It is initialized like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python">conversation_sum_bufw <span class="token operator">=</span> ConversationChain<span class="token punctuation">(</span>
    llm<span class="token operator">=</span>llm<span class="token punctuation">,</span> memory<span class="token operator">=</span>ConversationSummaryBufferMemory<span class="token punctuation">(</span>
        llm<span class="token operator">=</span>llm<span class="token punctuation">,</span>
        max_token_limit<span class="token operator">=</span><span class="token number">650</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>When applying this to our earlier conversation, we can set <code>max_token_limit</code> to a small number and yet the LLM can remember our earlier “aim”.</p><p>This is because that information is captured by the “summarization” component of the memory, despite being missed by the “buffer window” component.</p><p>Naturally, the pros and cons of this component are a mix of the earlier components on which this is based.</p><table><thead><tr><th>Pros</th><th>Cons</th></tr></thead><tbody><tr><td>Summarizer means we can remember distant interactions</td><td>Summarizer increases token count for shorter conversations</td></tr><tr><td>Buffer prevents us from missing information from the most recent interactions</td><td>Storing the raw interactions — even if just the most recent interactions — increases token count</td></tr></tbody></table><p>Although requiring more tweaking on what to summarize and what to maintain within the buffer window, the <code>ConversationSummaryBufferMemory</code> does give us plenty of flexibility and is the only one of our memory types (so far) that allows us to remember distant interactions <em>and</em> store the most recent interactions in their raw — and most information-rich — form.</p><p><img loading="lazy" src="./langchain3_files/langchain-conversational-memory-5.png" alt="Full comparison of different conversational memory types and effect on token count" width="100%">
<small>Token count comparisons including the <code>ConversationSummaryBufferMemory</code> type with <code>max_token_limit</code> values of <code>650</code> and <code>1300</code>.</small></p><p>We can also see that despite including a summary of past interactions <em>and</em> the raw form of recent interactions — the increase in token count of <code>ConversationSummaryBufferMemory</code> is competitive with other methods.</p><h3 id="other-memory-types">Other Memory Types</h3><p>The memory types we have covered here are great for getting started and give a good balance between remembering as much as possible and minimizing tokens.</p><p>However, we have other options — particularly the <code>ConversationKnowledgeGraphMemory</code> and <code>ConversationEntityMemory</code>. We’ll give these different forms of memory the attention they deserve in upcoming chapters.</p><hr><p>That’s it for this introduction to conversational memory for LLMs using LangChain. As we’ve seen, there are plenty of options for helping <em>stateless</em> LLMs interact as if they were in a <em>stateful</em> environment — able to consider and refer back to past interactions.</p><p>As mentioned, there are other forms of memory we can cover. We can also implement our own memory modules, use multiple types of memory within the same chain, combine them with agents, and much more. All of which we will cover in future chapters.</p><hr><a href="https://www.pinecone.io/learn/langchain-retrieval-augmentation/" class="btn next-chapter"><span>Next Chapter:</span><p>Fixing Hallucination with Knowledge Bases</p></a><section class="discourse-comments-section"><hr><h3>Comments</h3><div id="discourse-comments"><iframe src="./langchain3_files/comments.html" id="discourse-embed-frame" width="100%" frameborder="0" scrolling="no" referrerpolicy="no-referrer-when-downgrade" height="517px"></iframe></div></section><script type="text/javascript">const pageURL=window.location.protocol+'//'+window.location.host+window.location.pathname
DiscourseEmbed={discourseUrl:"https://community.pinecone.io/",discourseEmbedUrl:pageURL,};(function(){var d=document.createElement("script");d.type="text/javascript";d.async=true;d.src=DiscourseEmbed.discourseUrl+"javascripts/embed.js";(document.getElementsByTagName("head")[0]||document.getElementsByTagName("body")[0]).appendChild(d);})();</script></div><div class="post__sidebar post__sidebar--alpha nlp"><a class="book-title" href="https://www.pinecone.io/learn/langchain/"><img src="./langchain3_files/langchain-ebook.png" alt="LangChain AI Handbook by James Briggs and Francisco Ingham"><h3>LangChain AI Handbook</h3></a><p>Chapters:</p><ol class="chapter-list"><li class="chapter-title"><a href="https://www.pinecone.io/learn/langchain-intro/">LangChain: Introduction and Getting Started</a></li><li class="chapter-title"><a href="https://www.pinecone.io/learn/langchain-prompt-templates/">Prompt Engineering and LLMs with Langchain</a></li><li class="chapter-title semibold">Chatbot Memory with Langchain</li><div><nav id="TableOfContents"><ul><li><ul><li><a href="https://www.pinecone.io/learn/langchain-conversational-memory/#conversationchain">ConversationChain</a></li><li><a href="https://www.pinecone.io/learn/langchain-conversational-memory/#forms-of-conversational-memory">Forms of Conversational Memory</a></li></ul></li></ul></nav></div><li class="chapter-title"><a href="https://www.pinecone.io/learn/langchain-retrieval-augmentation/">Fixing Hallucination with Knowledge Bases</a></li><li class="chapter-title"><a href="https://www.pinecone.io/learn/langchain-agents/">Superpower LLMs with Conversational Agents in LangChain</a></li><li class="chapter-title"><a href="https://www.pinecone.io/learn/langchain-tools/">Building Custom Tools for LLM Agents</a></li></ol></div></div></div>