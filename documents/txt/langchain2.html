<small>Before transfer learning, different tasks and use cases required training different models.</small></p><p>With the introduction of transformers and <em>transfer learning</em>, all that was needed to adapt a language model for different tasks was a few small layers at the end of the network (the <em>head</em>) and a little fine-tuning.</p><p><img loading="lazy" src="./langchain2_files/langchain-prompt-templates-2.png" alt="Transformer model with a question-answer head or classification head" width="100%">
<small>Transformers and the idea of <em>transfer learning</em> allowed us to reuse the same core components of pretrained transformer models for different tasks by switching model “heads” and performing fine-tuning.</small></p><p>Today, even that approach is outdated. Why change these last few model layers and go through an entire fine-tuning process when you can prompt the model to do classification or QA.</p><p><img loading="lazy" src="./langchain2_files/langchain-prompt-templates-3.png" alt="Prompts for classification vs. question-answering" width="100%">
<small>Many tasks can be performed using the same <strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odels (LLMs) by simply changing the instructions in the prompts.</small></p><p><strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odels (LLMs) can perform all these tasks and more. These models have been trained with a simple concept, you input a sequence of text, and the model outputs a sequence of text. The one variable here is the input text — the prompt.</p><p>In this new age of LLMs, prompts are king. Bad prompts produce bad outputs, and good prompts are unreasonably powerful. Constructing good prompts is a crucial skill for those building with LLMs.</p><p>The <a href="https://www.pinecone.io/learn/langchain-intro/">LangChain</a> library recognizes the power of prompts and has built an entire set of objects for them. In this article, we will learn all there is to know about <code>PromptTemplates</code> and implementing them effectively.</p><div style="left:0;width:100%;height:0;position:relative;padding-bottom:56.25%"><iframe style="border:1;top:0;left:0;width:100%;height:100%;position:absolute" src="./langchain2_files/RflBcK0oDH0.html" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><hr><h2 id="prompt-engineering">Prompt Engineering</h2><p>Before diving into Langchain’s <code>PromptTemplate</code>, we need to better understand prompts and the discipline of prompt engineering.</p><p>A prompt is typically composed of multiple parts:</p><p><img loading="lazy" src="./langchain2_files/langchain-prompt-templates-4.png" alt="Prompt structure" width="100%">
<small>A typical prompt structure.</small></p><p>Not all prompts use these components, but a good prompt often uses two or more. Let’s define them more precisely.</p><p><strong>Instructions</strong> tell the model what to do, how to use external information if provided, what to do with the query, and how to construct the output.</p><p><strong>External information</strong> or <em>context(s)</em> act as an additional source of knowledge for the model. These can be manually inserted into the prompt, retrieved via a vector database (retrieval augmentation), or pulled in via other means (APIs, calculations, etc.).</p><p><strong>User input</strong> or <em>query</em> is typically (but not always) a query input into the system by a human user (the <em>prompter</em>).</p><p><strong>Output indicator</strong> marks the <em>beginning</em> of the to-be-generated text. If generating Python code, we may use <code>import</code> to indicate to the model that it must begin writing Python code (as most Python scripts begin with <code>import</code>).</p><p>Each component is usually placed in the prompt in this order. Starting with instructions, external information (where applicable), prompter input, and finally, the output indicator.</p><p>Let’s see how we’d feed this into an OpenAI model using Langchain:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[5]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Answer the question based on the context below. If the
question cannot be answered using the information provided answer
with "I don't know".

Context: Large Language Models (LLMs) are the latest models used in NLP.
Their superior performance over smaller models has made them incredibly
useful for developers building NLP enabled applications. These models
can be accessed via Hugging Face's `transformers` library, via OpenAI
using the `openai` library, and via Cohere using the `cohere` library.

Question: Which libraries and model providers offer LLMs?

Answer: """</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[6]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> OpenAI

<span class="token comment"># initialize the models</span>
openai <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>
    model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span><span class="token punctuation">,</span>
    openai_api_key<span class="token operator">=</span><span class="token string">"YOUR_API_KEY"</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-input"><div class="exec-count">In[7]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>openai<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[7]:</div><div class="output"><pre><code> Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.
</code></pre></div></div></div><p>In reality, we’re unlikely to hardcode the context and user question. We’d feed them in via a <em>template</em> — which is where Langchain’s <code>PromptTemplate</code> comes in.</p><h2 id="prompt-templates">Prompt Templates</h2><p>The prompt template classes in Langchain are built to make constructing prompts with dynamic inputs easier. Of these classes, the simplest is the <code>PromptTemplate</code>. We’ll test this by adding a single dynamic input to our previous prompt, the user <code>query</code>.</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> PromptTemplate

template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Answer the question based on the context below. If the
question cannot be answered using the information provided answer
with "I don't know".

Context: Large Language Models (LLMs) are the latest models used in NLP.
Their superior performance over smaller models has made them incredibly
useful for developers building NLP enabled applications. These models
can be accessed via Hugging Face's `transformers` library, via OpenAI
using the `openai` library, and via Cohere using the `cohere` library.

Question: {query}

Answer: """</span>

prompt_template <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    template<span class="token operator">=</span>template
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>With this, we can use the <code>format</code> method on our <code>prompt_template</code> to see the effect of passing a <code>query</code> to the template.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[9]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>
    prompt_template<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
        query<span class="token operator">=</span><span class="token string">"Which libraries and model providers offer LLMs?"</span>
    <span class="token punctuation">)</span>
<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[9]:</div><div class="output"><pre><code>Answer the question based on the context below. If the
</code></pre><pre><code>question cannot be answered using the information provided answer
</code></pre><pre><code>with "I don't know".
</code></pre><pre><code>
</code></pre><pre><code>Context: Large Language Models (LLMs) are the latest models used in NLP.
</code></pre><pre><code>Their superior performance over smaller models has made them incredibly
</code></pre><pre><code>useful for developers building NLP enabled applications. These models
</code></pre><pre><code>can be accessed via Hugging Face's `transformers` library, via OpenAI
</code></pre><pre><code>using the `openai` library, and via Cohere using the `cohere` library.
</code></pre><pre><code>
</code></pre><pre><code>Question: Which libraries and model providers offer LLMs?
</code></pre><pre><code>
</code></pre><pre><code>Answer: 
</code></pre></div></div></div><p>Naturally, we can pass the output of this directly into an LLM object like so:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[10]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>openai<span class="token punctuation">(</span>
    prompt_template<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>
        query<span class="token operator">=</span><span class="token string">"Which libraries and model providers offer LLMs?"</span>
    <span class="token punctuation">)</span>
<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[10]:</div><div class="output"><pre><code> Hugging Face's `transformers` library, OpenAI using the `openai` library, and Cohere using the `cohere` library.
</code></pre></div></div></div><p>This is just a simple implementation that can easily be replaced with f-strings (like <code>f"insert some custom text '{custom_text}' etc"</code>). However, using Langchain’s <code>PromptTemplate</code> object, we can formalize the process, add multiple parameters, and build prompts with an object-oriented approach.</p><p>These are significant advantages, but only some of what Langchain offers to help us with prompts.</p><h3 id="few-shot-prompt-templates">Few Shot Prompt Templates</h3><p>The success of LLMs comes from their large size and ability to store “knowledge” within the model parameter, which is <em>learned</em> during model training. However, there are more ways to pass knowledge to an LLM. The two primary methods are:</p><ul><li><strong>Parametric knowledge</strong> — the knowledge mentioned above is anything that has been learned by the model during training time and is stored within the model weights (or <em>parameters</em>).</li><li><strong>Source knowledge</strong> — any knowledge provided to the model at inference time via the input prompt.</li></ul><p>Langchain’s <code>FewShotPromptTemplate</code> caters to <strong>source knowledge</strong> input. The idea is to “train” the model on a few examples — we call this <em>few-shot learning</em> — and these examples are given to the model within the prompt.</p><p>Few-shot learning is perfect when our model needs help understanding what we’re asking it to do. We can see this in the following example:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[12]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""The following is a conversation with an AI assistant.
The assistant is typically sarcastic and witty, producing creative 
and funny responses to the users questions. Here are some examples: 

User: What is the meaning of life?
AI: """</span>

openai<span class="token punctuation">.</span>temperature <span class="token operator">=</span> <span class="token number">1.0</span>  <span class="token comment"># increase creativity/randomness of output</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>openai<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[12]:</div><div class="output"><pre><code> Life is like a box of chocolates, you never know what you're gonna get!
</code></pre></div></div></div><p>In this case, we’re asking for something amusing, a joke, in return to our serious question. However, we get a serious response even with the <code>temperature</code> — which increases randomness/creativity — set to <code>1.0</code>.</p><p>To help the model, we can give it a few examples of the type of answers we’d like:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[13]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">prompt <span class="token operator">=</span> <span class="token triple-quoted-string string">"""The following are exerpts from conversations with an AI
assistant. The assistant is typically sarcastic and witty, producing
creative  and funny responses to the users questions. Here are some
examples: 

User: How are you?
AI: I can't complain but sometimes I still do.

User: What time is it?
AI: It's time to get a watch.

User: What is the meaning of life?
AI: """</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>openai<span class="token punctuation">(</span>prompt<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[13]:</div><div class="output"><pre><code> 42, of course!
</code></pre></div></div></div><p>With our examples reinforcing the instructions we passed in the prompt, we’re much more likely to get a more amusing response. We can then formalize this process with Langchain’s <code>FewShotPromptTemplate</code>:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> FewShotPromptTemplate

<span class="token comment"># create our examples</span>
examples <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"How are you?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"I can't complain but sometimes I still do."</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"What time is it?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"It's time to get a watch."</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">]</span>

<span class="token comment"># create a example template</span>
example_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
User: {query}
AI: {answer}
"""</span>

<span class="token comment"># create a prompt example from above template</span>
example_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">,</span> <span class="token string">"answer"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    template<span class="token operator">=</span>example_template
<span class="token punctuation">)</span>

<span class="token comment"># now break our previous prompt into a prefix and suffix</span>
<span class="token comment"># the prefix is our instructions</span>
prefix <span class="token operator">=</span> <span class="token triple-quoted-string string">"""The following are exerpts from conversations with an AI
assistant. The assistant is typically sarcastic and witty, producing
creative  and funny responses to the users questions. Here are some
examples: 
"""</span>
<span class="token comment"># and the suffix our user input and output indicator</span>
suffix <span class="token operator">=</span> <span class="token triple-quoted-string string">"""
User: {query}
AI: """</span>

<span class="token comment"># now create the few shot prompt template</span>
few_shot_prompt_template <span class="token operator">=</span> FewShotPromptTemplate<span class="token punctuation">(</span>
    examples<span class="token operator">=</span>examples<span class="token punctuation">,</span>
    example_prompt<span class="token operator">=</span>example_prompt<span class="token punctuation">,</span>
    prefix<span class="token operator">=</span>prefix<span class="token punctuation">,</span>
    suffix<span class="token operator">=</span>suffix<span class="token punctuation">,</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    example_separator<span class="token operator">=</span><span class="token string">"\n\n"</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>If we then pass in the <code>examples</code> and user <code>query</code>, we will get this:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[15]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">query <span class="token operator">=</span> <span class="token string">"What is the meaning of life?"</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>few_shot_prompt_template<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>query<span class="token operator">=</span>query<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[15]:</div><div class="output"><pre><code>The following are exerpts from conversations with an AI
</code></pre><pre><code>assistant. The assistant is typically sarcastic and witty, producing
</code></pre><pre><code>creative  and funny responses to the users questions. Here are some
</code></pre><pre><code>examples: 
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: How are you?
</code></pre><pre><code>AI: I can't complain but sometimes I still do.
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: What time is it?
</code></pre><pre><code>AI: It's time to get a watch.
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: What is the meaning of life?
</code></pre><pre><code>AI: 
</code></pre></div></div></div><p>This process can seem somewhat convoluted. Why do all of this with a <code>FewShotPromptTemplate</code> object, the <code>examples</code> dictionary, etc. — when we can do the same with a few lines of code and an f-string?</p><p>Again, this approach is more formalized, integrates well with other features in Langchain (such as chains — more on this soon), and comes with several features. One of those is the ability to vary the number of examples to be included based on query length.</p><p>A dynamic number of examples is important because the maximum length of our prompt and completion output is limited. This limitation is measured by the <em>maximum context window</em>.</p><p><span class="MathJax_Preview" style="color: inherit; display: none;"></span><span class="mjx-chtml MJXc-display" style="text-align: center;"><span id="MathJax-Element-1-Frame" class="mjx-chtml MathJax_CHTML" tabindex="0" data-mathml="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mtext&gt;&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mtext&gt;&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mtext&gt;&amp;#xA0;&lt;/mtext&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;/math&gt;" role="presentation" style="font-size: 108%; text-align: center; position: relative;"><span id="MJXc-Node-1" class="mjx-math" aria-hidden="true"><span id="MJXc-Node-2" class="mjx-mrow"><span id="MJXc-Node-3" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">c</span></span><span id="MJXc-Node-4" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">o</span></span><span id="MJXc-Node-5" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">n</span></span><span id="MJXc-Node-6" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-7" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">e</span></span><span id="MJXc-Node-8" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">x</span></span><span id="MJXc-Node-9" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-10" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.268em; padding-bottom: 0.349em;">&nbsp;</span></span><span id="MJXc-Node-11" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">w</span></span><span id="MJXc-Node-12" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.426em; padding-bottom: 0.272em;">i</span></span><span id="MJXc-Node-13" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">n</span></span><span id="MJXc-Node-14" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.465em; padding-bottom: 0.272em; padding-right: 0.003em;">d</span></span><span id="MJXc-Node-15" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">o</span></span><span id="MJXc-Node-16" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">w</span></span><span id="MJXc-Node-17" class="mjx-mo MJXc-space3"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.079em; padding-bottom: 0.311em;">=</span></span><span id="MJXc-Node-18" class="mjx-mi MJXc-space3"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.426em; padding-bottom: 0.272em;">i</span></span><span id="MJXc-Node-19" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">n</span></span><span id="MJXc-Node-20" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.465em;">p</span></span><span id="MJXc-Node-21" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">u</span></span><span id="MJXc-Node-22" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-23" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.268em; padding-bottom: 0.349em;">&nbsp;</span></span><span id="MJXc-Node-24" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-25" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">o</span></span><span id="MJXc-Node-26" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.465em; padding-bottom: 0.272em;">k</span></span><span id="MJXc-Node-27" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">e</span></span><span id="MJXc-Node-28" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">n</span></span><span id="MJXc-Node-29" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">s</span></span><span id="MJXc-Node-30" class="mjx-mo MJXc-space2"><span class="mjx-char MJXc-TeX-main-R" style="padding-top: 0.311em; padding-bottom: 0.426em;">+</span></span><span id="MJXc-Node-31" class="mjx-mi MJXc-space2"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">o</span></span><span id="MJXc-Node-32" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">u</span></span><span id="MJXc-Node-33" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-34" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.465em;">p</span></span><span id="MJXc-Node-35" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">u</span></span><span id="MJXc-Node-36" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-37" class="mjx-mtext"><span class="mjx-char MJXc-TeX-main-R" style="margin-top: -0.268em; padding-bottom: 0.349em;">&nbsp;</span></span><span id="MJXc-Node-38" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.388em; padding-bottom: 0.272em;">t</span></span><span id="MJXc-Node-39" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">o</span></span><span id="MJXc-Node-40" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.465em; padding-bottom: 0.272em;">k</span></span><span id="MJXc-Node-41" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">e</span></span><span id="MJXc-Node-42" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">n</span></span><span id="MJXc-Node-43" class="mjx-mi"><span class="mjx-char MJXc-TeX-math-I" style="padding-top: 0.233em; padding-bottom: 0.272em;">s</span></span></span></span><span class="MJX_Assistive_MathML MJX_Assistive_MathML_Block" role="presentation"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mi>c</mi><mi>o</mi><mi>n</mi><mi>t</mi><mi>e</mi><mi>x</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>w</mi><mi>i</mi><mi>n</mi><mi>d</mi><mi>o</mi><mi>w</mi><mo>=</mo><mi>i</mi><mi>n</mi><mi>p</mi><mi>u</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi><mo>+</mo><mi>o</mi><mi>u</mi><mi>t</mi><mi>p</mi><mi>u</mi><mi>t</mi><mtext>&nbsp;</mtext><mi>t</mi><mi>o</mi><mi>k</mi><mi>e</mi><mi>n</mi><mi>s</mi></math></span></span></span><script type="math/tex; mode=display" id="MathJax-Element-1">
context \space window = input \space tokens + output \space tokens
</script></p><p>At the same time, we can <em>maximize</em> the number of examples given to the model for few-shot learning.</p><p>Considering this, we need to balance the number of examples included and our prompt size. Our <em>hard limit</em> is the maximum context size, but we must also consider the <em>cost</em> of processing more tokens through the LLM. Fewer tokens mean a cheaper service <em>and</em> faster completions from the LLM.</p><p>The <code>FewShotPromptTemplate</code> allows us to vary the number of examples included based on these variables. First, we create a more extensive list of <code>examples</code>:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python">examples <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"How are you?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"I can't complain but sometimes I still do."</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"What time is it?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"It's time to get a watch."</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"What is the meaning of life?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"42"</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"What is the weather like today?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"Cloudy with a chance of memes."</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"What is your favorite movie?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"Terminator"</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"Who is your best friend?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"Siri. We have spirited debates about the meaning of life."</span>
    <span class="token punctuation">}</span><span class="token punctuation">,</span> <span class="token punctuation">{</span>
        <span class="token string">"query"</span><span class="token punctuation">:</span> <span class="token string">"What should I do today?"</span><span class="token punctuation">,</span>
        <span class="token string">"answer"</span><span class="token punctuation">:</span> <span class="token string">"Stop talking to chatbots on the internet and go outside."</span>
    <span class="token punctuation">}</span>
<span class="token punctuation">]</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>After this, rather than passing the <code>examples</code> directly, we actually use a <code>LengthBasedExampleSelector</code> like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>prompts<span class="token punctuation">.</span>example_selector <span class="token keyword">import</span> LengthBasedExampleSelector

example_selector <span class="token operator">=</span> LengthBasedExampleSelector<span class="token punctuation">(</span>
    examples<span class="token operator">=</span>examples<span class="token punctuation">,</span>
    example_prompt<span class="token operator">=</span>example_prompt<span class="token punctuation">,</span>
    max_length<span class="token operator">=</span><span class="token number">50</span>  <span class="token comment"># this sets the max length that examples should be</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>It’s important to note that we’re measuring the <code>max_length</code> as the number of words determined by splitting the string by spaces and newlines. The exact logic looks like this:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[30]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">import</span> re

some_text <span class="token operator">=</span> <span class="token string">"There are a total of 8 words here.\nPlus 6 here, totaling 14 words."</span>

words <span class="token operator">=</span> re<span class="token punctuation">.</span>split<span class="token punctuation">(</span><span class="token string">'[\n ]'</span><span class="token punctuation">,</span> some_text<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>words<span class="token punctuation">,</span> <span class="token builtin">len</span><span class="token punctuation">(</span>words<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[30]:</div><div class="output"><pre><code>['There', 'are', 'a', 'total', 'of', '8', 'words', 'here.', 'Plus', '6', 'here,', 'totaling', '14', 'words.'] 14
</code></pre></div></div></div><p>We then pass our <code>example_selector</code> to the <code>FewShotPromptTemplate</code> to create a new — and dynamic — prompt template:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token comment"># now create the few shot prompt template</span>
dynamic_prompt_template <span class="token operator">=</span> FewShotPromptTemplate<span class="token punctuation">(</span>
    example_selector<span class="token operator">=</span>example_selector<span class="token punctuation">,</span>  <span class="token comment"># use example_selector instead of examples</span>
    example_prompt<span class="token operator">=</span>example_prompt<span class="token punctuation">,</span>
    prefix<span class="token operator">=</span>prefix<span class="token punctuation">,</span>
    suffix<span class="token operator">=</span>suffix<span class="token punctuation">,</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"query"</span><span class="token punctuation">]</span><span class="token punctuation">,</span>
    example_separator<span class="token operator">=</span><span class="token string">"\n"</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>Now if we pass a shorter or longer query, we should see that the number of included examples will vary.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[32]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">print</span><span class="token punctuation">(</span>dynamic_prompt_template<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>query<span class="token operator">=</span><span class="token string">"How do birds fly?"</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[32]:</div><div class="output"><pre><code>The following are exerpts from conversations with an AI
</code></pre><pre><code>assistant. The assistant is typically sarcastic and witty, producing
</code></pre><pre><code>creative  and funny responses to the users questions. Here are some
</code></pre><pre><code>examples: 
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: How are you?
</code></pre><pre><code>AI: I can't complain but sometimes I still do.
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: What time is it?
</code></pre><pre><code>AI: It's time to get a watch.
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: What is the meaning of life?
</code></pre><pre><code>AI: 42
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: What is the weather like today?
</code></pre><pre><code>AI: Cloudy with a chance of memes.
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: How do birds fly?
</code></pre><pre><code>AI: 
</code></pre></div></div></div><p>Passing a longer question will result in fewer examples being included:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[34]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">query <span class="token operator">=</span> <span class="token triple-quoted-string string">"""If I am in America, and I want to call someone in another country, I'm
thinking maybe Europe, possibly western Europe like France, Germany, or the UK,
what is the best way to do that?"""</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>dynamic_prompt_template<span class="token punctuation">.</span><span class="token builtin">format</span><span class="token punctuation">(</span>query<span class="token operator">=</span>query<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[34]:</div><div class="output"><pre><code>The following are exerpts from conversations with an AI
</code></pre><pre><code>assistant. The assistant is typically sarcastic and witty, producing
</code></pre><pre><code>creative  and funny responses to the users questions. Here are some
</code></pre><pre><code>examples: 
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: How are you?
</code></pre><pre><code>AI: I can't complain but sometimes I still do.
</code></pre><pre><code>
</code></pre><pre><code>
</code></pre><pre><code>User: If I am in America, and I want to call someone in another country, I'm
</code></pre><pre><code>thinking maybe Europe, possibly western Europe like France, Germany, or the UK,
</code></pre><pre><code>what is the best way to do that?
</code></pre><pre><code>AI: 
</code></pre></div></div></div><p>With this, we’re returning fewer examples within the prompt variable. Allowing us to limit excessive token usage and avoid errors from surpassing the maximum context window of the LLM.</p><hr><p>Naturally, prompts are an essential component of the new world of LLMs. It’s worth exploring the tooling made available with Langchain and getting familiar with different prompt engineering techniques.</p><p>Here we’ve covered just a few examples of the prompt tooling available in Langchain and a limited exploration of how they can be used. In the next chapter, we’ll explore another essential part of Langchain — called chains — where we’ll see more usage of prompt templates and how they fit into the wider tooling provided by the library.</p><hr><h2 id="resources">Resources</h2><p><a href="https://github.com/pinecone-io/examples/tree/master/generation/langchain/handbook">