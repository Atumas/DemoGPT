<!DOCTYPE html>
<!-- saved from url=(0046)https://www.pinecone.io/learn/langchain-intro/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./langchain1_files/fbevents.js"></script><script type="text/javascript" async="" src="./langchain1_files/insight.min.js"></script><script type="text/javascript" async="" src="./langchain1_files/js"></script><script type="text/javascript" async="" src="./langchain1_files/heap-3879495182.js"></script><script type="text/javascript" async="" src="./langchain1_files/analytics.js"></script><script src="./langchain1_files/fb.js" type="text/javascript" id="hs-ads-pixel-8231564" data-ads-portal-id="8231564" data-ads-env="prod" data-loader="hs-scriptloader" data-hsjs-portal="8231564" data-hsjs-env="prod" data-hsjs-hublet="na1"></script><script src="./langchain1_files/leadflows.js" type="text/javascript" id="LeadFlows-8231564" crossorigin="anonymous" data-leadin-portal-id="8231564" data-leadin-env="prod" data-loader="hs-scriptloader" data-hsjs-portal="8231564" data-hsjs-env="prod" data-hsjs-hublet="na1"></script><script src="./langchain1_files/8231564.js" type="text/javascript" id="cookieBanner-8231564" data-cookieconsent="ignore" data-hs-ignore="true" data-loader="hs-scriptloader" data-hsjs-portal="8231564" data-hsjs-env="prod" data-hsjs-hublet="na1"></script><script src="./langchain1_files/8231564(1).js" type="text/javascript" id="hs-analytics"></script><script src="./langchain1_files/pixel.js" async=""></script><script type="text/javascript" async="" src="./langchain1_files/insight.min.js"></script><script async="" src="./langchain1_files/gtm.js"></script><script type="text/javascript" async="" src="./langchain1_files/js(1)"></script><script type="text/javascript" async="" src="./langchain1_files/js(2)"></script><script src="./langchain1_files/optimize.js"></script><script>window.dataLayer=window.dataLayer||[];</script><title>LangChain: Introduction and Getting Started | Pinecone</title><meta name="viewport" content="width=device-width,initial-scale=1"><meta name="description" content="Introduction to the Generative AI and LLM framework for building apps with OpenAI&#39;s GPT-3 and open-source alternatives."><meta property="og:title" content="LangChain: Introduction and Getting Started | Pinecone"><meta property="og:description" content="Introduction to the Generative AI and LLM framework for building apps with OpenAI&#39;s GPT-3 and open-source alternatives."><meta property="og:type" content="article"><meta property="og:url" content="https://www.pinecone.io/learn/langchain-intro/"><meta name="twitter:card" content="summary_large_image"><meta name="twitter:creator" content="@pinecone"><meta name="twitter:site" content="@pinecone"><meta name="twitter:title" content="LangChain: Introduction and Getting Started | Pinecone"><meta name="twitter:description" content="Introduction to the Generative AI and LLM framework for building apps with OpenAI&#39;s GPT-3 and open-source alternatives."><meta property="og:image" content="https://www.pinecone.io/images/langchain-intro-0.png"><meta name="twitter:image" content="https://www.pinecone.io/images/langchain-intro-0.png"><meta property="og:site_name" content="Pinecone"><link rel="canonical" href="https://www.pinecone.io/learn/langchain-intro/"><link rel="apple-touch-icon" sizes="152x152" href="https://www.pinecone.io/favicon/apple-touch-icon.png"><link rel="icon" type="image/png" sizes="32x32" href="https://www.pinecone.io/favicon/favicon-32x32.png"><link rel="icon" type="image/png" sizes="16x16" href="https://www.pinecone.io/favicon/favicon-16x16.png"><link rel="manifest" href="https://www.pinecone.io/favicon/site.webmanifest"><link rel="mask-icon" href="https://www.pinecone.io/favicon/safari-pinned-tab.svg" color="#5bbad5"><meta name="msapplication-TileColor" content="#031bb3"><meta name="theme-color" content="#000000"><link rel="dns-prefetch" href="https://fonts.gstatic.com/"><link rel="dns-prefetch" href="https://www.googletagmanager.com/"><link rel="dns-prefetch" href="https://www.google-analytics.com/"><link rel="dns-prefetch" href="https://www.googleadservices.com/"><link rel="dns-prefetch" href="https://googleads.g.doubleclick.net/"><link rel="dns-prefetch" href="https://code.jquery.com/"><link rel="dns-prefetch" href="https://js.hs-scripts.com/"><link rel="dns-prefetch" href="https://js.hsleadflows.net/"><link rel="dns-prefetch" href="https://js.hs-analytics.net/"><link rel="dns-prefetch" href="https://js.hs-banner.com/"><link rel="dns-prefetch" href="https://js.hsadspixel.net/"><link rel="dns-prefetch" href="https://connect.facebook.net/"><link rel="dns-prefetch" href="https://cdn.jsdelivr.net/"><link rel="dns-prefetch" href="https://snap.licdn.com/"><link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin=""><link rel="preconnect" href="https://www.googletagmanager.com/" crossorigin=""><link rel="preconnect" href="https://www.google-analytics.com/" crossorigin=""><link rel="preconnect" href="https://www.googleadservices.com/" crossorigin=""><link rel="preconnect" href="https://googleads.g.doubleclick.net/" crossorigin=""><link rel="preconnect" href="https://code.jquery.com/" crossorigin=""><link rel="preconnect" href="https://js.hs-scripts.com/" crossorigin=""><link rel="preconnect" href="https://js.hsleadflows.net/" crossorigin=""><link rel="preconnect" href="https://js.hs-analytics.net/" crossorigin=""><link rel="preconnect" href="https://js.hs-banner.com/" crossorigin=""><link rel="preconnect" href="https://js.hsadspixel.net/" crossorigin=""><link rel="preconnect" href="https://connect.facebook.net/" crossorigin=""><link rel="preconnect" href="https://cdn.jsdelivr.net/" crossorigin=""><link rel="preconnect" href="https://snap.licdn.com/" crossorigin=""><link rel="preload" as="style" href="./langchain1_files/css2"><link href="./langchain1_files/css2" rel="stylesheet" media="all" onload="media=&quot;all&quot;"><link rel="stylesheet" href="./langchain1_files/style2.min.42bbae615a7a76e5d88a2a01157353b5afc629ac275539fb3a694908245b0216.css"><link rel="stylesheet" href="./langchain1_files/prism.min.7fd41fbbbc8c79b90cfc8e4e6e85bfdeb5538dd1a2c39151941c9d6452fefa16.css" media="all" onload="media=&quot;all&quot;"><script async="" src="./langchain1_files/js(3)"></script><script>window.branch='master';function gtag(){dataLayer.push(arguments);}
gtag('js',new Date());gtag('config','UA-155294302-1',{send_page_view:false,'custom_map':{'dimension1':'branch'}});gtag('event','page_view',{page_title:'LangChain: Introduction and Getting Started',page_location:'https:\/\/www.pinecone.io\/https:\/\/www.pinecone.io\/learn\/langchain-intro\/',page_path:'\/learn\/langchain-intro\/',send_to:'UA-155294302-1','branch':window.branch})
gtag('config','AW-451745872');</script><script>(function(w,d,s,l,i){w[l]=w[l]||[];w[l].push({'gtm.start':new Date().getTime(),event:'gtm.js'});var f=d.getElementsByTagName(s)[0],j=d.createElement(s),dl=l!='dataLayer'?'&l='+l:'';j.async=true;j.src='https://www.googletagmanager.com/gtm.js?id='+i+dl;f.parentNode.insertBefore(j,f);})(window,document,'script','dataLayer','GTM-5RCSPVG');</script><script type="text/javascript" async="" src="./langchain1_files/embed(1).js"></script><meta http-equiv="origin-trial" content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9"><meta http-equiv="origin-trial" content="A+xK4jmZTgh1KBVry/UZKUE3h6Dr9HPPioFS4KNCzify+KEoOii7z/goKS2zgbAOwhpZ1GZllpdz7XviivJM9gcAAACFeyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiQXR0cmlidXRpb25SZXBvcnRpbmdDcm9zc0FwcFdlYiIsImV4cGlyeSI6MTcwNzI2Mzk5OSwiaXNUaGlyZFBhcnR5Ijp0cnVlfQ=="><meta http-equiv="origin-trial" content="AymqwRC7u88Y4JPvfIF2F37QKylC04248hLCdJAsh8xgOfe/dVJPV3XS3wLFca1ZMVOtnBfVjaCMTVudWM//5g4AAAB7eyJvcmlnaW4iOiJodHRwczovL3d3dy5nb29nbGV0YWdtYW5hZ2VyLmNvbTo0NDMiLCJmZWF0dXJlIjoiUHJpdmFjeVNhbmRib3hBZHNBUElzIiwiZXhwaXJ5IjoxNjk1MTY3OTk5LCJpc1RoaXJkUGFydHkiOnRydWV9"><script type="text/javascript" async="" src="./langchain1_files/f.txt"></script><style type="text/css">.MathJax_Hover_Frame {border-radius: .25em; -webkit-border-radius: .25em; -moz-border-radius: .25em; -khtml-border-radius: .25em; box-shadow: 0px 0px 15px #83A; -webkit-box-shadow: 0px 0px 15px #83A; -moz-box-shadow: 0px 0px 15px #83A; -khtml-box-shadow: 0px 0px 15px #83A; border: 1px solid #A6D ! important; display: inline-block; position: absolute}
.MathJax_Menu_Button .MathJax_Hover_Arrow {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 4px; -webkit-border-radius: 4px; -moz-border-radius: 4px; -khtml-border-radius: 4px; font-family: 'Courier New',Courier; font-size: 9px; color: #F0F0F0}
.MathJax_Menu_Button .MathJax_Hover_Arrow span {display: block; background-color: #AAA; border: 1px solid; border-radius: 3px; line-height: 0; padding: 4px}
.MathJax_Hover_Arrow:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_Hover_Arrow:hover span {background-color: #CCC!important}
</style><style type="text/css">#MathJax_About {position: fixed; left: 50%; width: auto; text-align: center; border: 3px outset; padding: 1em 2em; background-color: #DDDDDD; color: black; cursor: default; font-family: message-box; font-size: 120%; font-style: normal; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 15px; -webkit-border-radius: 15px; -moz-border-radius: 15px; -khtml-border-radius: 15px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_About.MathJax_MousePost {outline: none}
.MathJax_Menu {position: absolute; background-color: white; color: black; width: auto; padding: 5px 0px; border: 1px solid #CCCCCC; margin: 0; cursor: default; font: menu; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; z-index: 201; border-radius: 5px; -webkit-border-radius: 5px; -moz-border-radius: 5px; -khtml-border-radius: 5px; box-shadow: 0px 10px 20px #808080; -webkit-box-shadow: 0px 10px 20px #808080; -moz-box-shadow: 0px 10px 20px #808080; -khtml-box-shadow: 0px 10px 20px #808080; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
.MathJax_MenuItem {padding: 1px 2em; background: transparent}
.MathJax_MenuArrow {position: absolute; right: .5em; padding-top: .25em; color: #666666; font-size: .75em}
.MathJax_MenuActive .MathJax_MenuArrow {color: white}
.MathJax_MenuArrow.RTL {left: .5em; right: auto}
.MathJax_MenuCheck {position: absolute; left: .7em}
.MathJax_MenuCheck.RTL {right: .7em; left: auto}
.MathJax_MenuRadioCheck {position: absolute; left: .7em}
.MathJax_MenuRadioCheck.RTL {right: .7em; left: auto}
.MathJax_MenuLabel {padding: 1px 2em 3px 1.33em; font-style: italic}
.MathJax_MenuRule {border-top: 1px solid #DDDDDD; margin: 4px 3px}
.MathJax_MenuDisabled {color: GrayText}
.MathJax_MenuActive {background-color: #606872; color: white}
.MathJax_MenuDisabled:focus, .MathJax_MenuLabel:focus {background-color: #E8E8E8}
.MathJax_ContextMenu:focus {outline: none}
.MathJax_ContextMenu .MathJax_MenuItem:focus {outline: none}
#MathJax_AboutClose {top: .2em; right: .2em}
.MathJax_Menu .MathJax_MenuClose {top: -10px; left: -10px}
.MathJax_MenuClose {position: absolute; cursor: pointer; display: inline-block; border: 2px solid #AAA; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; font-family: 'Courier New',Courier; font-size: 24px; color: #F0F0F0}
.MathJax_MenuClose span {display: block; background-color: #AAA; border: 1.5px solid; border-radius: 18px; -webkit-border-radius: 18px; -moz-border-radius: 18px; -khtml-border-radius: 18px; line-height: 0; padding: 8px 0 6px}
.MathJax_MenuClose:hover {color: white!important; border: 2px solid #CCC!important}
.MathJax_MenuClose:hover span {background-color: #CCC!important}
.MathJax_MenuClose:hover:focus {outline: none}
</style><style type="text/css">.MathJax_Preview .MJXf-math {color: inherit!important}
</style><style type="text/css">.MJX_Assistive_MathML {position: absolute!important; top: 0; left: 0; clip: rect(1px, 1px, 1px, 1px); padding: 1px 0 0 0!important; border: 0!important; height: 1px!important; width: 1px!important; overflow: hidden!important; display: block!important; -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none}
.MJX_Assistive_MathML.MJX_Assistive_MathML_Block {width: 100%!important}
</style><style type="text/css">#MathJax_Zoom {position: absolute; background-color: #F0F0F0; overflow: auto; display: block; z-index: 301; padding: .5em; border: 1px solid black; margin: 0; font-weight: normal; font-style: normal; text-align: left; text-indent: 0; text-transform: none; line-height: normal; letter-spacing: normal; word-spacing: normal; word-wrap: normal; white-space: nowrap; float: none; -webkit-box-sizing: content-box; -moz-box-sizing: content-box; box-sizing: content-box; box-shadow: 5px 5px 15px #AAAAAA; -webkit-box-shadow: 5px 5px 15px #AAAAAA; -moz-box-shadow: 5px 5px 15px #AAAAAA; -khtml-box-shadow: 5px 5px 15px #AAAAAA; filter: progid:DXImageTransform.Microsoft.dropshadow(OffX=2, OffY=2, Color='gray', Positive='true')}
#MathJax_ZoomOverlay {position: absolute; left: 0; top: 0; z-index: 300; display: inline-block; width: 100%; height: 100%; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
#MathJax_ZoomFrame {position: relative; display: inline-block; height: 0; width: 0}
#MathJax_ZoomEventTrap {position: absolute; left: 0; top: 0; z-index: 302; display: inline-block; border: 0; padding: 0; margin: 0; background-color: white; opacity: 0; filter: alpha(opacity=0)}
</style><style type="text/css">.MathJax_Preview {color: #888}
#MathJax_Message {position: fixed; left: 1em; bottom: 1.5em; background-color: #E6E6E6; border: 1px solid #959595; margin: 0px; padding: 2px 8px; z-index: 102; color: black; font-size: 80%; width: auto; white-space: nowrap}
#MathJax_MSIE_Frame {position: absolute; top: 0; left: 0; width: 0px; z-index: 101; border: 0px; margin: 0px; padding: 0px}
.MathJax_Error {color: #CC0000; font-style: italic}
</style><style type="text/css">.MJXp-script {font-size: .8em}
.MJXp-right {-webkit-transform-origin: right; -moz-transform-origin: right; -ms-transform-origin: right; -o-transform-origin: right; transform-origin: right}
.MJXp-bold {font-weight: bold}
.MJXp-italic {font-style: italic}
.MJXp-scr {font-family: MathJax_Script,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-frak {font-family: MathJax_Fraktur,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-sf {font-family: MathJax_SansSerif,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-cal {font-family: MathJax_Caligraphic,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-mono {font-family: MathJax_Typewriter,'Times New Roman',Times,STIXGeneral,serif}
.MJXp-largeop {font-size: 150%}
.MJXp-largeop.MJXp-int {vertical-align: -.2em}
.MJXp-math {display: inline-block; line-height: 1.2; text-indent: 0; font-family: 'Times New Roman',Times,STIXGeneral,serif; white-space: nowrap; border-collapse: collapse}
.MJXp-display {display: block; text-align: center; margin: 1em 0}
.MJXp-math span {display: inline-block}
.MJXp-box {display: block!important; text-align: center}
.MJXp-box:after {content: " "}
.MJXp-rule {display: block!important; margin-top: .1em}
.MJXp-char {display: block!important}
.MJXp-mo {margin: 0 .15em}
.MJXp-mfrac {margin: 0 .125em; vertical-align: .25em}
.MJXp-denom {display: inline-table!important; width: 100%}
.MJXp-denom > * {display: table-row!important}
.MJXp-surd {vertical-align: top}
.MJXp-surd > * {display: block!important}
.MJXp-script-box > *  {display: table!important; height: 50%}
.MJXp-script-box > * > * {display: table-cell!important; vertical-align: top}
.MJXp-script-box > *:last-child > * {vertical-align: bottom}
.MJXp-script-box > * > * > * {display: block!important}
.MJXp-mphantom {visibility: hidden}
.MJXp-munderover {display: inline-table!important}
.MJXp-over {display: inline-block!important; text-align: center}
.MJXp-over > * {display: block!important}
.MJXp-munderover > * {display: table-row!important}
.MJXp-mtable {vertical-align: .25em; margin: 0 .125em}
.MJXp-mtable > * {display: inline-table!important; vertical-align: middle}
.MJXp-mtr {display: table-row!important}
.MJXp-mtd {display: table-cell!important; text-align: center; padding: .5em 0 0 .5em}
.MJXp-mtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-mlabeledtr {display: table-row!important}
.MJXp-mlabeledtr > .MJXp-mtd:first-child {padding-left: 0}
.MJXp-mlabeledtr:first-child > .MJXp-mtd {padding-top: 0}
.MJXp-merror {background-color: #FFFF88; color: #CC0000; border: 1px solid #CC0000; padding: 1px 3px; font-style: normal; font-size: 90%}
.MJXp-scale0 {-webkit-transform: scaleX(.0); -moz-transform: scaleX(.0); -ms-transform: scaleX(.0); -o-transform: scaleX(.0); transform: scaleX(.0)}
.MJXp-scale1 {-webkit-transform: scaleX(.1); -moz-transform: scaleX(.1); -ms-transform: scaleX(.1); -o-transform: scaleX(.1); transform: scaleX(.1)}
.MJXp-scale2 {-webkit-transform: scaleX(.2); -moz-transform: scaleX(.2); -ms-transform: scaleX(.2); -o-transform: scaleX(.2); transform: scaleX(.2)}
.MJXp-scale3 {-webkit-transform: scaleX(.3); -moz-transform: scaleX(.3); -ms-transform: scaleX(.3); -o-transform: scaleX(.3); transform: scaleX(.3)}
.MJXp-scale4 {-webkit-transform: scaleX(.4); -moz-transform: scaleX(.4); -ms-transform: scaleX(.4); -o-transform: scaleX(.4); transform: scaleX(.4)}
.MJXp-scale5 {-webkit-transform: scaleX(.5); -moz-transform: scaleX(.5); -ms-transform: scaleX(.5); -o-transform: scaleX(.5); transform: scaleX(.5)}
.MJXp-scale6 {-webkit-transform: scaleX(.6); -moz-transform: scaleX(.6); -ms-transform: scaleX(.6); -o-transform: scaleX(.6); transform: scaleX(.6)}
.MJXp-scale7 {-webkit-transform: scaleX(.7); -moz-transform: scaleX(.7); -ms-transform: scaleX(.7); -o-transform: scaleX(.7); transform: scaleX(.7)}
.MJXp-scale8 {-webkit-transform: scaleX(.8); -moz-transform: scaleX(.8); -ms-transform: scaleX(.8); -o-transform: scaleX(.8); transform: scaleX(.8)}
.MJXp-scale9 {-webkit-transform: scaleX(.9); -moz-transform: scaleX(.9); -ms-transform: scaleX(.9); -o-transform: scaleX(.9); transform: scaleX(.9)}
.MathJax_PHTML .noError {vertical-align: ; font-size: 90%; text-align: left; color: black; padding: 1px 3px; border: 1px solid}
</style><style type="text/css">
@font-face {
  font-weight: 400;
  font-style:  normal;
  font-family: circular;

  src: url('chrome-extension://liecbddmkiiihnedobmlmillhodjkdmb/fonts/CircularXXWeb-Book.woff2') format('woff2');
}

@font-face {
  font-weight: 700;
  font-style:  normal;
  font-family: circular;

  src: url('chrome-extension://liecbddmkiiihnedobmlmillhodjkdmb/fonts/CircularXXWeb-Bold.woff2') format('woff2');
}</style><script async="" src="./langchain1_files/js(4)"></script><script type="text/javascript" async="" src="./langchain1_files/f(1).txt"></script></head><body data-new-gr-c-s-check-loaded="14.1112.0" data-gr-ext-installed="" class="vsc-initialized"><style type="text/css">div#hs-eu-cookie-confirmation{background:#fff;height:auto;left:0;position:absolute;top:0;width:100%;z-index:100000000!important;border-bottom:1px solid #cbd6e2;border-top:1px solid #cbd6e2;box-shadow:0 1px 5px #eaf0f6;color:#33475b;font-family:inherit;font-size:inherit;font-weight:400!important;line-height:inherit;text-align:left;text-shadow:none!important;font-size:12px;font-family:Helvetica Neue,Helvetica,Arial,sans-serif;line-height:18px}div#hs-eu-cookie-confirmation.hs-cookie-notification-position-bottom{position:fixed;border-bottom:0;bottom:0;top:auto;box-shadow:0 -1px 3px #eaf0f6}div#hs-eu-cookie-confirmation *{box-sizing:border-box!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner{background:#fff;margin:0 auto;max-width:1000px;padding:20px}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a{text-decoration:none!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a,div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a:hover{background:none!important;border:none!important;box-shadow:none!important;color:#0091ae;font-family:inherit;font-size:inherit;font-weight:400!important;line-height:inherit;text-align:left;text-shadow:none!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a:hover{text-decoration:underline!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner div#hs-eu-policy-wording{margin-bottom:12px}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner div#hs-en-cookie-confirmation-buttons-area,div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner div#hs-eu-cookie-confirmation-button-group{display:flex;flex-direction:row;flex-wrap:wrap;align-items:center}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner div#hs-en-cookie-confirmation-buttons-area{margin-right:72px;justify-content:flex-end;align-items:center}@media (max-width:800px){div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner div#hs-en-cookie-confirmation-buttons-area{justify-content:center;margin-right:0}}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner div#hs-eu-cookie-confirmation-button-group{justify-content:center}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-confirmation-button,div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-cookie-settings-button,div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-decline-button{margin:6px!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-confirmation-button,div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-decline-button{border-radius:3px;display:inline-block;padding:10px 16px!important;text-decoration:none!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-confirmation-button{background-color:#041BB3!important;border:1px solid #041BB3!important;color:#fff;font-family:inherit;font-size:inherit;font-weight:400!important;line-height:inherit;text-align:left;text-shadow:none!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-decline-button{border:1px solid #041BB3!important;color:#041BB3;font-family:inherit;font-size:inherit;font-weight:400!important;line-height:inherit;text-align:left;text-shadow:none!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-cookie-settings-button{color:#041BB3!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner p{margin:0 72px 12px;color:#33475b;font-family:inherit;font-size:inherit;font-weight:400!important;line-height:inherit;text-align:left;text-shadow:none!important}@media (max-width:800px){div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner p{margin:0 20px 12px}}#hs-eu-close-button-container{display:flex;justify-content:end;margin-top:8px;margin-right:8px}#hs-eu-close-button-container a#hs-eu-close-button[role=button]{float:right;width:1.5rem;font-size:40px!important;text-align:center!important;cursor:pointer;color:#8b8589!important}#hs-eu-close-button-container a#hs-eu-close-button[role=button]:hover{background:none!important;border:none!important;box-shadow:none!important;color:#0091ae;font-family:inherit;font-size:inherit;font-weight:400!important;line-height:inherit;text-align:left;text-shadow:none!important;text-decoration:none!important}@media (max-width:800px){#hs-eu-close-button-container a#hs-eu-close-button[role=button]{margin-right:10px;font-size:30px;line-height:50px}}@media (min-width:800px){#hs-eu-close-button-container a#hs-eu-close-button[role=button]{margin-bottom:10px}}@media print{div#hs-eu-cookie-confirmation{display:none!important}}@media screen and (max-width:480px){div#hs-eu-cookie-confirmation{font-size:12px!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner{padding:8px 14px 14px!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a,div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner a#hs-eu-confirmation-button{font-size:12px!important}div#hs-eu-cookie-confirmation div#hs-eu-cookie-confirmation-inner p{font-size:12px!important;margin-bottom:12px!important;line-height:15px!important}}@media only screen and (min-width:960px){div#hs-eu-cookie-confirmation{position:fixed}}</style><div id="hs-eu-cookie-confirmation" class="hs-cookie-notification-position-bottom" data-nosnippet="">
            <div id="hs-eu-cookie-confirmation-inner">
              
              <div id="hs-eu-policy-wording"><p>This website stores cookies on your computer. These cookies are used to collect information about how you interact with our website and allow us to remember you. We use this information in order to improve and customize your browsing experience and for analytics and metrics about our visitors both on this website and other media. To find out more about the cookies we use, see <a href="https://www.pinecone.io/cookies/" target="_blank">Cookie Policy</a> and <a href="https://www.pinecone.io/privacy/" target="_blank">Privacy Policy</a>.</p></div>
              
              <div id="hs-en-cookie-confirmation-buttons-area">
                
                <div id="hs-eu-cookie-confirmation-button-group">
                <a href="javascript:void(0);" id="hs-eu-confirmation-button" role="button">
    Accept
  </a>
                
                </div>
              </div>
            </div>
          </div><div id="MathJax_Message" style="display: none;"></div><header><nav class="main-nav" style="top: 51px;"><ul class="inner desktop"><li><a class="logo" href="https://www.pinecone.io/"><img src="./langchain1_files/pinecone-logo.svg" alt="Pinecone" width="123" height="26"></a></li><li><a class="navbar-item" href="https://www.pinecone.io/pricing/">Pricing</a></li><li class="dropdown-item"><span>Build<svg enable-background="new 0 0 29 14" width="10" viewBox="0 0 29 14"><polygon fill="#000" points="0.15,0 14.5,14.35 28.85,0"></polygon></svg></span>
<span class="dropdown-content"><a class="navbar-item" href="https://www.pinecone.io/docs/">Documentation</a>
<a class="navbar-item" href="https://www.pinecone.io/docs/api/">API Reference</a>
<a class="navbar-item" href="https://www.pinecone.io/docs/examples/">Examples</a>
<a class="navbar-item" href="https://support.pinecone.io/">Support Center</a>
<a class="navbar-item" href="https://community.pinecone.io/">Support Forum</a></span></li><li class="dropdown-item"><span>Learn<svg enable-background="new 0 0 29 14" width="10" viewBox="0 0 29 14"><polygon fill="#000" points="0.15,0 14.5,14.35 28.85,0"></polygon></svg></span>
<span class="dropdown-content"><a class="navbar-item" href="https://www.pinecone.io/learn/">Learning Center</a>
<a class="navbar-item" href="https://www.pinecone.io/showcase/">Showcase</a>
<a class="navbar-item" href="https://www.pinecone.io/community/">Community</a>
<a class="navbar-item" href="https://www.pinecone.io/learn/vector-database/">What's a vector database?</a></span></li><li class="dropdown-item"><span>Company<svg enable-background="new 0 0 29 14" width="10" viewBox="0 0 29 14"><polygon fill="#000" points="0.15,0 14.5,14.35 28.85,0"></polygon></svg></span>
<span class="dropdown-content"><a class="navbar-item" href="https://www.pinecone.io/company/">About</a>
<a class="navbar-item" href="https://www.pinecone.io/careers/">Careers</a>
<a class="navbar-item" href="https://www.pinecone.io/partners/">Partners</a>
<a class="navbar-item" href="https://www.pinecone.io/newsroom/">Newsroom</a>
<a class="navbar-item" href="https://www.pinecone.io/security/">Trust &amp; Security</a></span></li><li><a class="navbar-item" href="https://www.pinecone.io/contact/">Contact</a></li><li><a class="nav-hiring-link" href="https://www.pinecone.io/careers/">We're hiring!</a></li><li><a href="https://app.pinecone.io/?sessionType=login">Log In</a></li><li><a class="btn" href="https://app.pinecone.io/?sessionType=signup">Sign Up Free</a></li></ul><ul class="inner mobile"><div class="nav-cta"><a class="btn outline" href="https://app.pinecone.io/?sessionType=login">Sign In</a><div class="divider"></div><a class="btn" href="https://app.pinecone.io/?sessionType=signup">Create Account</a></div><li><a href="https://www.pinecone.io/pricing/">Pricing</a></li><li><a href="https://www.pinecone.io/learn/langchain-intro/">Build<span class="wedge"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 10" width="10" height="10"><path d="M1 1 9 5 1 9z" fill="#000"></path></svg></span></a><ul class="sub-menu"><li><a href="https://www.pinecone.io/docs">Documentation</a></li><li><a href="https://www.pinecone.io/docs/api/">API Reference</a></li><li><a href="https://www.pinecone.io/docs/examples/">Examples</a></li><li><a href="https://support.pinecone.io/">Support Center</a></li><li><a href="https://community.pinecone.io/">Support Forum</a></li></ul></li><li><a href="https://www.pinecone.io/learn/langchain-intro/">Learn<span class="wedge"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 10" width="10" height="10"><path d="M1 1 9 5 1 9z" fill="currentcolor"></path></svg></span></a><ul class="sub-menu"><li><a href="https://www.pinecone.io/learn/">Learning Center</a></li><li><a href="https://www.pinecone.io/showcase/">Showcase</a></li><li><a href="https://www.pinecone.io/community/">Community</a></li><li><a href="https://www.pinecone.io/learn/vector-database/">What is a Vector Database?</a></li></ul></li><li><a href="https://www.pinecone.io/learn/langchain-intro/">Company<span class="wedge"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 10 10" width="10" height="10"><path d="M1 1 9 5 1 9z" fill="currentcolor"></path></svg></span></a><ul class="sub-menu"><li><a href="https://www.pinecone.io/company/">About</a></li><li><a href="https://www.pinecone.io/careers/">Careers</a></li><li><a href="https://www.pinecone.io/partners/">Partners</a></li><li><a href="https://www.pinecone.io/newsroom/">Newsroom</a></li><li><a href="https://www.pinecone.io/security/">Trust &amp; Security</a></li></ul></li><li><a href="https://www.pinecone.io/contact/">Contact</a></li><div class="divider horizontal"></div><li><a class="btn highlight" href="https://www.pinecone.io/careers/">We're Hiring!</a></li><div class="divider horizontal"></div></ul><button class="nav-toggle" onclick="toggleNav()">
<span class="vh">Toggle menu</span></button></nav><div class="nav-overlay mobile"></div></header><script>const navOverlay=document.querySelector(".nav-overlay")
const toggleNav=()=>{const nav=document.querySelector(".main-nav")
nav.classList.toggle("mobile")
navOverlay.style.display=nav.classList.contains("mobile")?"block":"none"}
navOverlay.addEventListener("click",function(){toggleNav();});document.querySelectorAll(".inner.mobile a").forEach(function(link){const subMenu=link.nextElementSibling;if(subMenu&&subMenu.classList.contains("sub-menu")){link.addEventListener("click",function(event){event.preventDefault();document.querySelectorAll(".inner.mobile .expanded").forEach(function(expandedItem){if(expandedItem!==link.parentNode){expandedItem.classList.remove("expanded");expandedItem.querySelector(".sub-menu").style.display="none";}});subMenu.style.display=subMenu.style.display==="flex"?"none":"flex";link.parentNode.classList.toggle("expanded");});}});</script><div id="sticky-bar" class="sticky-bar"><a href="https://summit.pinecone.io/"><div class="inner"><span class="sticky-bar-badge">ANNOUNCEMENT</span>
<span class="sticky-bar-text">Join us at the Pinecone Summit: AI Transformation without Hallucination - July 13th, 2023
<span class="sticky-bar-cta" style="margin-left:10px">Get Tickets<svg xmlns="http://www.w3.org/2000/svg" width="10" height="10" viewBox="8.59 3.59 12.24 18.83"><path d="M11.414063 3.585938 8.585938 6.414063 15.171875 13 8.585938 19.585938l2.828125 2.828125L20.828125 13z"></path></svg></span></span>
<button class="dismiss" aria-label="Hide banner">×</button></div></a></div><script>stickyBar();function stickyBar(){const header=document.querySelector(".main-nav");const stickyBar=document.querySelector("#sticky-bar");barCheck();function moveHeader(){header.style.top=`${stickyBar.clientHeight}px`;}
function barCheck(){const hidebar=sessionStorage.getItem("bar-hide")||false;if(hidebar){barHide();}else barEvents();}
function barEvents(){moveHeader();document.querySelector("#sticky-bar .dismiss").addEventListener("click",(e)=>{e.preventDefault();window.removeEventListener("resize",moveHeader);barPrevent();barHide();});window.addEventListener("resize",moveHeader);}
function barHide(){document.querySelector("#sticky-bar").remove();header.style.top="0";}
function barPrevent(){sessionStorage.setItem("bar-hide",true);}}</script><main><div class="hero2"><div class="inner"><div class="hero2__content2"></div></div></div><div class="post"><div class="post__inner nlp inner"><div class="post__content content__inner"><h1>LangChain: Introduction and Getting Started</h1><p><strong>L</strong>arge <strong>L</strong>anguage <strong>M</strong>odels (LLMs) entered the world stage with the release of OpenAI’s GPT-3 in 2020 [1]. Since then, they’ve enjoyed a steady growth in popularity.</p><p>That is until late 2022. Interest in LLMs and the broader discipline of generative AI has skyrocketed. The reasons for this are likely the continuous upward momentum of significant advances in LLMs.</p><p>We saw the dramatic news about Google’s <em>“sentient”</em> LaMDA chatbot. The first high-performance and <em>open-source</em> LLM called BLOOM was released. OpenAI released their next-generation text embedding model and the next generation of <em>“GPT-3.5”</em> models.</p><p>After all these giant leaps forward in the LLM space, OpenAI released <em>ChatGPT</em> — thrusting LLMs into the spotlight.</p><p><a href="https://github.com/hwchase17/langchain">LangChain</a> appeared around the same time. Its creator, Harrison Chase, made the first commit in late October 2022. Leaving a short couple of months of development before getting caught in the LLM wave.</p><p>Despite being early days for the library, it is already packed full of incredible features for building amazing tools around the core of LLMs. In this article, we’ll introduce the library and start with the most straightforward component offered by LangChain — LLMs.</p><div style="left:0;width:100%;height:0;position:relative;padding-bottom:56.25%"><iframe style="border:1;top:0;left:0;width:100%;height:100%;position:absolute" src="./langchain1_files/nE2skSRWTTs.html" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div><hr><h2 id="langchain">LangChain</h2><p>At its core, LangChain is a framework built around LLMs. We can use it for chatbots, <a href="https://www.pinecone.io/learn/openai-gen-qa/"><strong>G</strong>enerative <strong>Q</strong>uestion-<strong>A</strong>nswering (GQA)</a>, summarization, and much more.</p><p>The core idea of the library is that we can <em>“chain”</em> together different components to create more advanced use cases around LLMs. Chains may consist of multiple components from several modules:</p><ul><li><p><strong>Prompt templates</strong>: Prompt templates are templates for different types of prompts. Like “chatbot” style templates, ELI5 question-answering, etc</p></li><li><p><strong>LLMs</strong>: Large language models like GPT-3, BLOOM, etc</p></li><li><p><strong>Agents</strong>: Agents use LLMs to decide what actions should be taken. Tools like web search or calculators can be used, and all are packaged into a logical loop of operations.</p></li><li><p><strong>Memory</strong>: Short-term memory, long-term memory.</p></li></ul><p>We will dive into each of these in much more detail in upcoming chapters of the LangChain handbook. You can stay updated for each release via our newsletter:</p><div class="shortcode-form"><form class="form js-form" data-ga="Newsletter" data-id="74adb696-e56a-47b3-9911-f249db88a172" data-portal="8231564" method="post" name="newsletter"><input type="hidden" name="form-name" value="newsletter"><fieldset><div hidden=""><label>Don't fill this out if you're human: <input name="bot-field"></label></div><label class="shortcode-form-label" for="newsletter-email">Subscribe to stay updated with LangChain releases!</label>
<span class="form-group"><input type="email" name="email" id="newsletter-email" placeholder="Email address..." required="">
<button class="btn" type="submit">Submit</button></span>
<input type="hidden" name="lifecyclestage" id="lifecyclestage" value="subscriber"><div class="form__info"><div class="js-form-ok" role="alert" hidden="">Subscribed successfully.</div><div class="js-form-not-ok" role="alert" hidden="">Failed to submit.</div></div></fieldset></form></div><p>For now, we’ll start with the basics behind <strong>prompt templates</strong> and <strong>LLMs</strong>. We’ll also explore two LLM options available from the library, using models from <em>Hugging Face Hub</em> or <em>OpenAI</em>.</p><h2 id="our-first-prompt-templates">Our First Prompt Templates</h2><p>Prompts being input to LLMs are often structured in different ways so that we can get different results. For Q&amp;A, we could take a user’s question and reformat it for different Q&amp;A styles, like conventional Q&amp;A, a bullet list of answers, or even a summary of problems relevant to the given question.</p><h3 id="creating-prompts-in-langchain">Creating Prompts in LangChain</h3><p>Let’s put together a simple question-answering prompt template. We first need to install the <code>langchain</code> library.</p><pre><code>!pip install langchain
</code></pre><hr><p><em>Follow along with the code via <a href="https://colab.research.google.com/github/pinecone-io/examples/blob/master/generation/langchain/handbook/00-langchain-intro.ipynb">Colab</a>!</em></p><hr><p>From here, we import the <code>PromptTemplate</code> class and initialize a template like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> PromptTemplate

template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Question: {question}

Answer: """</span>
prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>
        template<span class="token operator">=</span>template<span class="token punctuation">,</span>
    input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">'question'</span><span class="token punctuation">]</span>
<span class="token punctuation">)</span>

<span class="token comment"># user question</span>
question <span class="token operator">=</span> <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?"</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>When using these prompt template with the given <code>question</code> we will get:</p><pre><code>Question: Which NFL team won the Super Bowl in the 2010 season?

Answer: 
</code></pre><p>For now, that’s all we need. We’ll use the same prompt template across both Hugging Face Hub and OpenAI LLM generations.</p><h2 id="hugging-face-hub-llm">Hugging Face Hub LLM</h2><p>The Hugging Face Hub endpoint in LangChain connects to the Hugging Face Hub and runs the models via their free inference endpoints. We need a <a href="https://huggingface.co/settings/tokens">Hugging Face account and API key</a> to use these endpoints.</p><p>Once you have an API key, we add it to the <code>HUGGINGFACEHUB_API_TOKEN</code> environment variable. We can do this with Python like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">import</span> os

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'HUGGINGFACEHUB_API_TOKEN'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'HF_API_KEY'</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>Next, we must install the <code>huggingface_hub</code> library via Pip.</p><pre><code>!pip install huggingface_hub
</code></pre><p>Now we can generate text using a Hub model. We’ll use <a href="https://huggingface.co/google/flan-t5-xl"><code>google/flan-t5-x1</code></a>.</p><hr><p><em>The default Hugging Face Hub inference APIs do not use specialized hardware and, therefore, can be slow. They are also not suitable for running larger models like <code>bigscience/bloom-560m</code> or <code>google/flan-t5-xxl</code> (note <code>xxl</code> vs. <code>xl</code>).</em></p><hr><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[3]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python"><span class="token keyword">from</span> langchain <span class="token keyword">import</span> HuggingFaceHub<span class="token punctuation">,</span> LLMChain

<span class="token comment"># initialize Hub LLM</span>
hub_llm <span class="token operator">=</span> HuggingFaceHub<span class="token punctuation">(</span>
        repo_id<span class="token operator">=</span><span class="token string">'google/flan-t5-xl'</span><span class="token punctuation">,</span>
    model_kwargs<span class="token operator">=</span><span class="token punctuation">{</span><span class="token string">'temperature'</span><span class="token punctuation">:</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">10</span><span class="token punctuation">}</span>
<span class="token punctuation">)</span>

<span class="token comment"># create prompt template &gt; LLM chain</span>
llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>hub_llm
<span class="token punctuation">)</span>

<span class="token comment"># ask the user question about NFL 2010</span>
<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[3]:</div><div class="output"><pre><code>green bay packers
</code></pre></div></div></div><p>For this question, we get the correct answer of <code>"green bay packers"</code>.</p><h3 id="asking-multiple-questions">Asking Multiple Questions</h3><p>If we’d like to ask multiple questions, we can try two approaches:</p><ol><li>Iterate through all questions using the <code>generate</code> method, answering them one at a time.</li><li>Place all questions into a single prompt for the LLM; this will only work for more advanced LLMs.</li></ol><p>Starting with option (1), let’s see how to use the <code>generate</code> method:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[4]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">qs <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Who was the 12th person on the moon?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"How many eyes does a blade of grass have?"</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
res <span class="token operator">=</span> llm_chain<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>qs<span class="token punctuation">)</span>
res<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[4]:</div><div class="output"><pre><code>LLMResult(generations=[[Generation(text='green bay packers', generation_info=None)], [Generation(text='184', generation_info=None)], [Generation(text='john glenn', generation_info=None)], [Generation(text='one', generation_info=None)]], llm_output=None)</code></pre></div></div></div><p>Here we get bad results except for the first question. This is simply a limitation of the LLM being used.</p><p>If the model cannot answer individual questions accurately, grouping all queries into a single prompt is unlikely to work. However, for the sake of experimentation, let’s try it.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[6]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">multi_template <span class="token operator">=</span> <span class="token triple-quoted-string string">"""Answer the following questions one at a time.

Questions:
{questions}

Answers:
"""</span>
long_prompt <span class="token operator">=</span> PromptTemplate<span class="token punctuation">(</span>template<span class="token operator">=</span>multi_template<span class="token punctuation">,</span> input_variables<span class="token operator">=</span><span class="token punctuation">[</span><span class="token string">"questions"</span><span class="token punctuation">]</span><span class="token punctuation">)</span>

llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>long_prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>flan_t5
<span class="token punctuation">)</span>

qs_str <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?\n"</span> <span class="token operator">+</span>
    <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?\n"</span> <span class="token operator">+</span>
    <span class="token string">"Who was the 12th person on the moon?"</span> <span class="token operator">+</span>
    <span class="token string">"How many eyes does a blade of grass have?"</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>qs_str<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[6]:</div><div class="output"><pre><code>If I am 6 ft 4 inches, how tall am I in centimeters
</code></pre></div></div></div><p>As expected, the results are not helpful. We’ll see later that more powerful LLMs can do this.</p><h2 id="openai-llms">OpenAI LLMs</h2><p>The OpenAI endpoints in LangChain connect to OpenAI directly or via Azure. We need an <a href="https://beta.openai.com/account/api-keys">OpenAI account and API key</a> to use these endpoints.</p><p>Once you have an API key, we add it to the <code>OPENAI_API_TOKEN</code> environment variable. We can do this with Python like so:</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">import</span> os

os<span class="token punctuation">.</span>environ<span class="token punctuation">[</span><span class="token string">'OPENAI_API_TOKEN'</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token string">'OPENAI_API_KEY'</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><p>Next, we must install the <code>openai</code> library via Pip.</p><pre><code>!pip install openai
</code></pre><p>Now we can generate text using OpenAI’s GPT-3 generation (or <em>completion</em>) models. We’ll use <a href="https://huggingface.co/google/flan-t5-xl"><code>text-davinci-003</code></a>.</p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> OpenAI

davinci <span class="token operator">=</span> OpenAI<span class="token punctuation">(</span>model_name<span class="token operator">=</span><span class="token string">'text-davinci-003'</span><span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><hr><p><em>Alternatively, if you’re using OpenAI via Azure, you can do:</em></p><div class="highlight"><div class="code-toolbar"><pre class="language-python line-numbers" tabindex="0"><code class="language-python" data-lang="python"><span class="token keyword">from</span> langchain<span class="token punctuation">.</span>llms <span class="token keyword">import</span> AzureOpenAI

llm <span class="token operator">=</span> AzureOpenAI<span class="token punctuation">(</span>
    deployment_name<span class="token operator">=</span><span class="token string">"your-azure-deployment"</span><span class="token punctuation">,</span> 
    model_name<span class="token operator">=</span><span class="token string">"text-davinci-003"</span>
<span class="token punctuation">)</span>
<span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><hr><p>We’ll use the same simple question-answer prompt template as before with the Hugging Face example. The only change is that we now pass our OpenAI LLM <code>davinci</code>:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[15]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>davinci
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>question<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[15]:</div><div class="output"><pre><code> The Green Bay Packers won the Super Bowl in the 2010 season.
</code></pre></div></div></div><p>As expected, we’re getting the correct answer. We can do the same for multiple questions using <code>generate</code>:</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[16]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">qs <span class="token operator">=</span> <span class="token punctuation">[</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"Who was the 12th person on the moon?"</span><span class="token punctuation">}</span><span class="token punctuation">,</span>
    <span class="token punctuation">{</span><span class="token string">'question'</span><span class="token punctuation">:</span> <span class="token string">"How many eyes does a blade of grass have?"</span><span class="token punctuation">}</span>
<span class="token punctuation">]</span>
llm_chain<span class="token punctuation">.</span>generate<span class="token punctuation">(</span>qs<span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[16]:</div><div class="output"><pre><code>LLMResult(generations=[[Generation(text=' The Green Bay Packers won the Super Bowl in the 2010 season.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' 193.04 centimeters', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' Charles Duke was the 12th person on the moon. He was part of the Apollo 16 mission in 1972.', generation_info={'finish_reason': 'stop', 'logprobs': None})], [Generation(text=' A blade of grass does not have any eyes.', generation_info={'finish_reason': 'stop', 'logprobs': None})]], llm_output={'token_usage': {'total_tokens': 124, 'prompt_tokens': 75, 'completion_tokens': 49}})</code></pre></div></div></div><p>Most of our results are correct or have a degree of truth. The model undoubtedly functions better than the <code>google/flan-t5-xl</code> model. As before, let’s try feeding all questions into the model at once.</p><div class="jupyter-notebook" style="max-height:none"><div class="notebook-cell notebook-input"><div class="exec-count">In[17]:</div><div class="code-toolbar"><pre class="notebook no-line-numbers language-python line-numbers" tabindex="0"><code class="notebook language-python">llm_chain <span class="token operator">=</span> LLMChain<span class="token punctuation">(</span>
    prompt<span class="token operator">=</span>long_prompt<span class="token punctuation">,</span>
    llm<span class="token operator">=</span>davinci
<span class="token punctuation">)</span>

qs_str <span class="token operator">=</span> <span class="token punctuation">(</span>
    <span class="token string">"Which NFL team won the Super Bowl in the 2010 season?\n"</span> <span class="token operator">+</span>
    <span class="token string">"If I am 6 ft 4 inches, how tall am I in centimeters?\n"</span> <span class="token operator">+</span>
    <span class="token string">"Who was the 12th person on the moon?"</span> <span class="token operator">+</span>
    <span class="token string">"How many eyes does a blade of grass have?"</span>
<span class="token punctuation">)</span>

<span class="token keyword">print</span><span class="token punctuation">(</span>llm_chain<span class="token punctuation">.</span>run<span class="token punctuation">(</span>qs_str<span class="token punctuation">)</span><span class="token punctuation">)</span><span aria-hidden="true" class="line-numbers-rows"><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span><span></span></span></code></pre><div class="toolbar"><div class="toolbar-item"><button class="copy-to-clipboard-button" type="button" data-copy-state="copy"><span>Copy</span></button></div></div></div></div><div class="notebook-cell notebook-output"><div class="exec-count">Out[17]:</div><div class="output"><pre><code>The New Orleans Saints won the Super Bowl in the 2010 season.
</code></pre><pre><code>6 ft 4 inches is 193 centimeters.
</code></pre><pre><code>The 12th person on the moon was Harrison Schmitt.
</code></pre><pre><code>A blade of grass does not have eyes.
</code></pre></div></div></div><p>As we keep rerunning the query, the model will occasionally make errors, but at other times manage to get all answers correct.</p><hr><p>That’s it for our introduction to LangChain — a library that allows us to build more advanced apps around LLMs like OpenAI’s GPT-3 models or the open-source alternatives available via Hugging Face.</p><p>As mentioned, LangChain can do much more than we’ve demonstrated here. We’ll be covering these other features in upcoming articles.</p><hr>